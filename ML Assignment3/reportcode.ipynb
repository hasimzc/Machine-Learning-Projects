{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2661bf2e",
   "metadata": {},
   "source": [
    "In this assignment, we will try to determine which category a news article belongs to\n",
    "among five categories (Sport, Business, Politics, Entertainment, Tech)\n",
    "We will implement a Naive Bayes classifier and verify itâ€™s performance on English News Dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55300547",
   "metadata": {},
   "source": [
    "# Importing some libraries and showing first 5 row of dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8432ea4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ArticleId</th>\n",
       "      <th>Text</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1833</td>\n",
       "      <td>worldcom ex-boss launches defence lawyers defe...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>154</td>\n",
       "      <td>german business confidence slides german busin...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1101</td>\n",
       "      <td>bbc poll indicates economic gloom citizens in ...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1976</td>\n",
       "      <td>lifestyle  governs mobile choice  faster  bett...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>917</td>\n",
       "      <td>enron bosses in $168m payout eighteen former e...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ArticleId                                               Text  Category\n",
       "0       1833  worldcom ex-boss launches defence lawyers defe...  business\n",
       "1        154  german business confidence slides german busin...  business\n",
       "2       1101  bbc poll indicates economic gloom citizens in ...  business\n",
       "3       1976  lifestyle  governs mobile choice  faster  bett...      tech\n",
       "4        917  enron bosses in $168m payout eighteen former e...  business"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer,ENGLISH_STOP_WORDS\n",
    "\n",
    "dataset = pd.read_csv('English Dataset.csv')\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f723ac2e",
   "metadata": {},
   "source": [
    "# Part 1 Understanding the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7bc3668c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "business --> ('the', 7133) ('to', 3306) ('of', 2864) ('in', 2821) ('and', 2161) ('said', 1100) ('is', 1072) ('that', 1052) ('for', 1045) ('it', 1011) ('on', 906) ('has', 835) ('its', 736) ('by', 718) ('with', 612) ('at', 609) ('as', 605) ('was', 596) ('from', 588) ('be', 573) ('have', 554) ('are', 538) ('us', 522) ('will', 520) ('year', 456) ('which', 404) ('mr', 393) ('but', 391) ('an', 385) ('had', 343) ('this', 322) ('would', 309) ('been', 307) ('up', 306) ('not', 302) ('more', 298) ('he', 287) ('market', 284) ('were', 282) ('also', 279) ('than', 275) ('new', 273) ('their', 265) ('firm', 261) ('growth', 257) ('company', 253) ('last', 236) ('economy', 233) ('about', 228) ('after', 221)\n",
      "\n",
      "tech --> ('the', 7498) ('to', 4149) ('of', 3425) ('and', 3017) ('in', 2316) ('that', 1676) ('is', 1597) ('it', 1465) ('for', 1299) ('on', 1111) ('said', 1064) ('be', 1057) ('are', 994) ('as', 929) ('will', 797) ('with', 787) ('have', 731) ('by', 727) ('has', 686) ('was', 653) ('people', 647) ('they', 630) ('more', 624) ('not', 535) ('at', 533) ('but', 527) ('which', 514) ('from', 500) ('he', 484) ('can', 480) ('this', 474) ('or', 452) ('their', 422) ('up', 413) ('an', 408) ('its', 388) ('about', 371) ('one', 351) ('you', 351) ('mr', 349) ('new', 349) ('also', 348) ('were', 344) ('mobile', 343) ('than', 343) ('we', 336) ('would', 322) ('been', 312) ('could', 308) ('technology', 303)\n",
      "\n",
      "politics --> ('the', 7957) ('to', 3913) ('of', 2840) ('and', 2559) ('in', 2159) ('said', 1445) ('he', 1410) ('for', 1237) ('that', 1195) ('on', 1186) ('is', 1167) ('be', 1080) ('mr', 1073) ('was', 1013) ('it', 998) ('would', 712) ('as', 690) ('have', 669) ('but', 668) ('not', 662) ('by', 651) ('with', 645) ('will', 642) ('are', 635) ('has', 611) ('they', 572) ('at', 564) ('his', 540) ('labour', 494) ('from', 466) ('government', 464) ('had', 462) ('an', 429) ('election', 424) ('we', 416) ('this', 410) ('blair', 395) ('were', 379) ('been', 377) ('party', 376) ('people', 372) ('there', 364) ('their', 357) ('which', 340) ('who', 330) ('also', 308) ('more', 298) ('if', 289) ('minister', 286) ('up', 284)\n",
      "\n",
      "sport --> ('the', 6620) ('to', 3189) ('and', 2532) ('in', 2510) ('of', 1826) ('for', 1127) ('he', 1105) ('on', 1014) ('but', 992) ('is', 985) ('it', 974) ('was', 943) ('that', 863) ('have', 812) ('with', 803) ('at', 794) ('his', 762) ('we', 660) ('has', 650) ('said', 636) ('be', 614) ('will', 575) ('as', 547) ('not', 490) ('from', 481) ('after', 477) ('by', 430) ('had', 414) ('they', 414) ('their', 381) ('been', 363) ('are', 356) ('game', 356) ('an', 353) ('this', 353) ('out', 351) ('first', 350) ('year', 331) ('england', 329) ('who', 324) ('against', 312) ('time', 296) ('when', 295) ('win', 295) ('up', 294) ('two', 290) ('world', 269) ('all', 268) ('over', 267) ('there', 264)\n",
      "\n",
      "entertainment --> ('the', 5822) ('and', 2130) ('to', 2108) ('of', 2048) ('in', 1996) ('for', 1097) ('on', 881) ('was', 818) ('it', 738) ('is', 684) ('with', 662) ('said', 594) ('he', 584) ('film', 583) ('at', 554) ('be', 502) ('that', 491) ('as', 479) ('has', 471) ('by', 460) ('his', 460) ('will', 433) ('best', 430) ('have', 384) ('who', 358) ('from', 356) ('are', 331) ('but', 326) ('year', 315) ('which', 305) ('an', 299) ('also', 277) ('been', 276) ('this', 266) ('one', 265) ('us', 264) ('had', 263) ('were', 262) ('music', 255) ('not', 251) ('they', 241) ('she', 240) ('we', 236) ('their', 235) ('new', 234) ('up', 225) ('show', 222) ('her', 215) ('first', 188) ('after', 186)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# since ArticleId has no effect to classify, we ignored this column\n",
    "\n",
    "dataset.drop('ArticleId', inplace=True, axis=1) \n",
    "dataset = dataset.reset_index(drop=True) \n",
    "\n",
    "# distinct categories\n",
    "categories = dataset['Category'].unique()\n",
    "\n",
    "\n",
    "a = dataset.to_numpy()\n",
    "\n",
    "cat_business = []\n",
    "cat_tech = []\n",
    "cat_politics = []\n",
    "cat_sports = []\n",
    "cat_entertainment = []\n",
    "\n",
    "\n",
    "# collecting whole texts of each category\n",
    "whole_category = [cat_business, cat_tech, cat_politics, cat_sports, cat_entertainment]\n",
    "\n",
    "for i in range(len(a)):\n",
    "    if a[i][1] == categories[0]:\n",
    "        cat_business.append(a[i][0])\n",
    "        \n",
    "    elif a[i][1] == categories[1]:\n",
    "        cat_tech.append(a[i][0])\n",
    "        \n",
    "    elif a[i][1] == categories[2]:\n",
    "        cat_politics.append(a[i][0])\n",
    "        \n",
    "    elif a[i][1] == categories[3]:\n",
    "        cat_sports.append(a[i][0])\n",
    "        \n",
    "    elif a[i][1] == categories[4]:\n",
    "        cat_entertainment.append(a[i][0])\n",
    "        \n",
    "from collections import defaultdict\n",
    "#         \n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "for c in range(len(whole_category)):\n",
    "    \n",
    "    cat_name = whole_category[c] \n",
    "    \n",
    "    # for each category we create a matrix\n",
    "    X = vectorizer.fit_transform(cat_name) \n",
    "    matrix = X.toarray()\n",
    "\n",
    "    # collecting columns of matrix which gives words of each category         \n",
    "    columns = (vectorizer.get_feature_names_out())\n",
    "    \n",
    "    # purpose of the summation of the columns is finding the word which has most frequency\n",
    "    sumOfColumns = np.sum(matrix, axis=0)\n",
    "\n",
    "    # keeping words and number of words in dict_word for each category\n",
    "    dict_word = defaultdict(int)\n",
    "    print(categories[c] , '-->', end=' ')\n",
    "    \n",
    "    # finding most frequent words for each category\n",
    "\n",
    "    for i in range(50):\n",
    "\n",
    "        max_value = np.max(sumOfColumns)\n",
    "\n",
    "        for j in range(len(sumOfColumns)):\n",
    "            if sumOfColumns[j] == max_value:\n",
    "                dict_word[columns[j]] = max_value\n",
    "                \n",
    "        # result means index of first max summation\n",
    "        result = np.where(sumOfColumns == max_value)[0][0]\n",
    "        # the reason behind assinging result to zero is that not considering the same max value\n",
    "        sumOfColumns[result] = 0\n",
    "    print(*dict_word.items())\n",
    "    \n",
    "    print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb2e7b1",
   "metadata": {},
   "source": [
    "OUTPUT :\n",
    "\n",
    "business --> ('the', 7133) ('to', 3306) ('of', 2864) ('in', 2821) ('and', 2161) ('said', 1100) ('is', 1072) ('that', 1052) ('for', 1045) ('it', 1011) ('on', 906) ('has', 835) ('its', 736) ('by', 718) ('with', 612) ('at', 609) ('as', 605) ('was', 596) ('from', 588) ('be', 573) ('have', 554) ('are', 538) ('us', 522) ('will', 520) ('year', 456) ('which', 404) ('mr', 393) ('but', 391) ('an', 385) ('had', 343) ('this', 322) ('would', 309) ('been', 307) ('up', 306) ('not', 302) ('more', 298) ('he', 287) ('market', 284) ('were', 282) ('also', 279) ('than', 275) ('new', 273) ('their', 265) ('firm', 261) ('growth', 257) ('company', 253) ('last', 236) ('economy', 233) ('about', 228) ('after', 221)\n",
    "\n",
    "tech --> ('the', 7498) ('to', 4149) ('of', 3425) ('and', 3017) ('in', 2316) ('that', 1676) ('is', 1597) ('it', 1465) ('for', 1299) ('on', 1111) ('said', 1064) ('be', 1057) ('are', 994) ('as', 929) ('will', 797) ('with', 787) ('have', 731) ('by', 727) ('has', 686) ('was', 653) ('people', 647) ('they', 630) ('more', 624) ('not', 535) ('at', 533) ('but', 527) ('which', 514) ('from', 500) ('he', 484) ('can', 480) ('this', 474) ('or', 452) ('their', 422) ('up', 413) ('an', 408) ('its', 388) ('about', 371) ('one', 351) ('you', 351) ('mr', 349) ('new', 349) ('also', 348) ('were', 344) ('mobile', 343) ('than', 343) ('we', 336) ('would', 322) ('been', 312) ('could', 308) ('technology', 303)\n",
    "\n",
    "politics --> ('the', 7957) ('to', 3913) ('of', 2840) ('and', 2559) ('in', 2159) ('said', 1445) ('he', 1410) ('for', 1237) ('that', 1195) ('on', 1186) ('is', 1167) ('be', 1080) ('mr', 1073) ('was', 1013) ('it', 998) ('would', 712) ('as', 690) ('have', 669) ('but', 668) ('not', 662) ('by', 651) ('with', 645) ('will', 642) ('are', 635) ('has', 611) ('they', 572) ('at', 564) ('his', 540) ('labour', 494) ('from', 466) ('government', 464) ('had', 462) ('an', 429) ('election', 424) ('we', 416) ('this', 410) ('blair', 395) ('were', 379) ('been', 377) ('party', 376) ('people', 372) ('there', 364) ('their', 357) ('which', 340) ('who', 330) ('also', 308) ('more', 298) ('if', 289) ('minister', 286) ('up', 284)\n",
    "\n",
    "sport --> ('the', 6620) ('to', 3189) ('and', 2532) ('in', 2510) ('of', 1826) ('for', 1127) ('he', 1105) ('on', 1014) ('but', 992) ('is', 985) ('it', 974) ('was', 943) ('that', 863) ('have', 812) ('with', 803) ('at', 794) ('his', 762) ('we', 660) ('has', 650) ('said', 636) ('be', 614) ('will', 575) ('as', 547) ('not', 490) ('from', 481) ('after', 477) ('by', 430) ('had', 414) ('they', 414) ('their', 381) ('been', 363) ('are', 356) ('game', 356) ('an', 353) ('this', 353) ('out', 351) ('first', 350) ('year', 331) ('england', 329) ('who', 324) ('against', 312) ('time', 296) ('when', 295) ('win', 295) ('up', 294) ('two', 290) ('world', 269) ('all', 268) ('over', 267) ('there', 264)\n",
    "\n",
    "entertainment --> ('the', 5822) ('and', 2130) ('to', 2108) ('of', 2048) ('in', 1996) ('for', 1097) ('on', 881) ('was', 818) ('it', 738) ('is', 684) ('with', 662) ('said', 594) ('he', 584) ('film', 583) ('at', 554) ('be', 502) ('that', 491) ('as', 479) ('has', 471) ('by', 460) ('his', 460) ('will', 433) ('best', 430) ('have', 384) ('who', 358) ('from', 356) ('are', 331) ('but', 326) ('year', 315) ('which', 305) ('an', 299) ('also', 277) ('been', 276) ('this', 266) ('one', 265) ('us', 264) ('had', 263) ('were', 262) ('music', 255) ('not', 251) ('they', 241) ('she', 240) ('we', 236) ('their', 235) ('new', 234) ('up', 225) ('show', 222) ('her', 215) ('first', 188) ('after', 186)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753f9814",
   "metadata": {},
   "source": [
    "most meaningful 3 words of each category:\n",
    "\n",
    "total words of business is 112281\n",
    "Business -->  market : 284, firm : 261, growth : 257\n",
    "\n",
    "total words of Tech is 130985\n",
    "Tech --> people : 647, mobile : 343 technology : 303\n",
    "\n",
    "total words of Politics is 123215   \n",
    "Politics --> labour : 494, government : 464, election : 424\n",
    "  \n",
    "total words of Sports is 116030\n",
    "Sports --> game : 356, first : 350, england : 329 \n",
    "\n",
    "total words of Entertainment is 91158\n",
    "Entertainment --> film : 583, best : 430, music : 255 \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f240b2",
   "metadata": {},
   "source": [
    "# Part 2: Implementing Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1795f00b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram:  99.32885906040268\n",
      "Bigram:  100.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "dataset_array = dataset.to_numpy()\n",
    "\n",
    "np.random.shuffle(dataset_array) # shuffling dataset_array\n",
    "test = dataset_array[4 * len(dataset_array) // 5:].copy()\n",
    "training = dataset_array[:4 * len(dataset_array) // 5].copy()\n",
    "cat_business = []\n",
    "cat_tech = []\n",
    "cat_politics = []\n",
    "cat_sports = []\n",
    "cat_entertainment = []\n",
    "\n",
    "whole_category = [cat_business, cat_tech, cat_politics, cat_sports, cat_entertainment]\n",
    "\n",
    "# listing lengths of total words in every categories. \n",
    "length_of_words_categorically = [0,0,0,0,0]\n",
    "\n",
    "# collecting whole texts of each category in addition , finding total number of words of each category.\n",
    "\n",
    "\n",
    "for i in range(len(dataset_array)):\n",
    "    if dataset_array[i][1] == categories[0]:\n",
    "        cat_business.append(dataset_array[i][0])\n",
    "        length_of_words_categorically[0] += len(dataset_array[i][0].split())\n",
    "    elif dataset_array[i][1] == categories[1]:\n",
    "        cat_tech.append(dataset_array[i][0])\n",
    "        length_of_words_categorically[1] += len(dataset_array[i][0].split())\n",
    "    elif dataset_array[i][1] == categories[2]:\n",
    "        cat_politics.append(dataset_array[i][0])\n",
    "        length_of_words_categorically[2] += len(dataset_array[i][0].split())\n",
    "    elif dataset_array[i][1] == categories[3]:\n",
    "        cat_sports.append(dataset_array[i][0])\n",
    "        length_of_words_categorically[3] += len(dataset_array[i][0].split())\n",
    "    elif dataset_array[i][1] == categories[4]:\n",
    "        cat_entertainment.append(dataset_array[i][0])\n",
    "        length_of_words_categorically[4] += len(dataset_array[i][0].split())\n",
    "row = dataset.shape[0] # number of texts\n",
    "from collections import defaultdict\n",
    "vectorizer = CountVectorizer() # CountVectorizer for unigram matrix.\n",
    "vectorizer2 = CountVectorizer(ngram_range=(2, 2))# CountVectorizer for bigram matrix.\n",
    "whole_matrix = vectorizer.fit_transform(training[:, 0]).toarray()\n",
    "number_of_unique_words = whole_matrix.shape[1]\n",
    "whole_matrix_of_bigrams = vectorizer2.fit_transform(training[:, 0]).toarray()\n",
    "number_of_unique_bigrams = whole_matrix_of_bigrams.shape[1]\n",
    "bag_of_unigrams = defaultdict()\n",
    "bag_of_bigrams = defaultdict()\n",
    "# according to unigram and bigram matrices, creating dictionaries respectively\n",
    "for c in range(len(whole_category)):\n",
    "    cat_name = whole_category[c]\n",
    "    X = vectorizer.fit_transform(cat_name)\n",
    "    unigram_matrix = X.toarray()\n",
    "    words = (vectorizer.get_feature_names_out())\n",
    "    bigram_matrix = vectorizer2.fit_transform(cat_name).toarray()\n",
    "    bigrams = vectorizer2.get_feature_names_out()\n",
    "    vocabulary = defaultdict(int) # vocabulary of unigrams for each category\n",
    "    vocabulary2 = defaultdict(int) # vocabulary of bigrams for each category\n",
    "    sumOfColumns = np.sum(unigram_matrix, axis=0)\n",
    "    # adding existing words to unigram vocabulary\n",
    "    for i in range(unigram_matrix.shape[0]):\n",
    "        for j in range(unigram_matrix.shape[1]):\n",
    "            if unigram_matrix[i][j] == 0:\n",
    "                continue\n",
    "            else:\n",
    "                vocabulary[words[j]] += unigram_matrix[i][j]\n",
    "    # collecting unigram vocabularies for each category\n",
    "    bag_of_unigrams[categories[c]] = vocabulary\n",
    "    # adding existing words to bigram vocabulary\n",
    "    for i in range(bigram_matrix.shape[0]):\n",
    "        for j in range(bigram_matrix.shape[1]):\n",
    "            if bigram_matrix[i][j] == 0:\n",
    "                continue\n",
    "            else:\n",
    "                vocabulary2[bigrams[j]] += bigram_matrix[i][j]\n",
    "    # collecting bigram vocabularies for each category\n",
    "    bag_of_bigrams[categories[c]] = vocabulary2\n",
    "    \n",
    "def naive_bayes(unigrams,bigrams,test):\n",
    "    test = np.ndarray.tolist(test)\n",
    "    import re\n",
    "    global number_of_unique_words, length_of_words_categorically\n",
    "    for i in range(len(test)):\n",
    "        test[i] = test[i][0].lower() # converting every letter to lowercase \n",
    "        test[i] = re.sub(r'[^a-zA-Z0-9]', ' ', test[i]) # we are removing punctuations for not considering them as a word\n",
    "        test[i] = test[i].split() # converting text string to string list of every word.\n",
    "    predictions_unigram = []\n",
    "    predictions_bigram = []\n",
    "    for i in range(len(test)):\n",
    "        scores_unigram = [0,0,0,0,0] # keeping scores of each category for unigram\n",
    "        scores_bigram = [0,0,0,0,0] # keeping scores of each category for bigram\n",
    "        for j in range(len(test[i])):\n",
    "            for k in range(5):\n",
    "                if test[i][j] in unigrams[categories[k]].keys():\n",
    "                    scores_unigram[k] += np.log((unigrams[categories[k]][test[i][j]] +1) / (length_of_words_categorically[k] + number_of_unique_words))\n",
    "                else:\n",
    "                    scores_unigram[k] += np.log(1 / (length_of_words_categorically[k] + number_of_unique_words))\n",
    "                if j+2 < len(test[i]):\n",
    "                    bigram = test[i][j] + ' ' + test[i][j+1]\n",
    "                    if bigram in bigrams[categories[k]].keys():\n",
    "                        scores_bigram[k] += np.log((bigrams[categories[k]][bigram] + 1) / (length_of_words_categorically[k] + number_of_unique_words))\n",
    "                    else:\n",
    "                        scores_bigram[k] += np.log(1/(length_of_words_categorically[k]+number_of_unique_words))\n",
    "\n",
    "        predictions_unigram.append(categories[scores_unigram.index(max(scores_unigram))])\n",
    "        predictions_bigram.append(categories[scores_bigram.index(max(scores_bigram))])\n",
    "    return predictions_unigram , predictions_bigram\n",
    "\n",
    "def accuracy(outputs,predictions):\n",
    "    correct_predictions = 0\n",
    "    for i in range(len(outputs)):\n",
    "        if outputs[i] == predictions[i]:\n",
    "            correct_predictions +=1\n",
    "    return 100 * correct_predictions / len(outputs)\n",
    "\n",
    "predictions = naive_bayes(bag_of_unigrams,bag_of_bigrams,test)\n",
    "print(\"Unigram: \",accuracy(test[:,1],predictions[0]))\n",
    "print(\"Bigram: \",accuracy(test[:,1],predictions[1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037e2778",
   "metadata": {},
   "source": [
    "OUTPUT:\n",
    "    \n",
    "Unigram:  99.66442953020135\n",
    "\n",
    "Bigram:  100.0\n",
    "\n",
    "OUTPUT-2:\n",
    "\n",
    "Unigram:  98.99328859060402\n",
    "\n",
    "Bigram:  99.66442953020135\n",
    "\n",
    "After every running,\n",
    "implementation of naive bayes for bigram, classifies better than Unigram\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a75dcb7",
   "metadata": {},
   "source": [
    "# Part 3 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67d381c",
   "metadata": {},
   "source": [
    "# Listing 10 words whose presence most strongly predicts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53efb058",
   "metadata": {},
   "source": [
    "# First Perspective of Listing 10 words whose presence most strongly predicts\n",
    "\n",
    "We removed words if their idf is equal to 1, because meaning of idf is 1, the word exists in each text of that category so that word not too specific and strong. \n",
    "Combination of tf-idf and idf to get 10 most strongly predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "013e6c70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "business :  ['in', 'and', 'said', 'that', 'is', 'it', 'for', 'on', 'has', 'its']\n",
      "tech :  ['of', 'in', 'that', 'is', 'it', 'for', 'said', 'on', 'be', 'are']\n",
      "politics :  ['of', 'and', 'he', 'said', 'mr', 'for', 'on', 'that', 'is', 'be']\n",
      "sport :  ['to', 'in', 'and', 'of', 'he', 'is', 'we', 'for', 'it', 'on']\n",
      "entertainment :  ['to', 'and', 'in', 'of', 'for', 'film', 'on', 'was', 'it', 'he']\n"
     ]
    }
   ],
   "source": [
    "vectorizer3 = TfidfVectorizer()\n",
    "specific_word_for_each_category = []\n",
    "for c in range(len(whole_category)):\n",
    "    vectorizer3 = TfidfVectorizer()\n",
    "    X = vectorizer3.fit_transform(whole_category[c])\n",
    "    idfs = vectorizer3.idf_  \n",
    "\n",
    "    wordList = vectorizer3.get_feature_names_out()\n",
    "\n",
    "    tfidf_matrix = X.toarray()\n",
    "\n",
    "    specific_word = []\n",
    "    # to find the most frequent words \n",
    "    sumOfCol = np.sum(X.toarray(), axis=0)\n",
    "\n",
    "    for k in range(15):  # this range should be at least the length of the list (10)\n",
    "        max_val = np.max(sumOfCol)\n",
    "\n",
    "        for i in range(len(wordList)):\n",
    "            # meaning of idf is 1, the word exists in each text of that category. \n",
    "            # That's why we remove this word which is not too strong to predict the class \n",
    "            if idfs[i] == 1:\n",
    "                sumOfCol[i] = 0\n",
    "            if max_val != 0:\n",
    "                # after finding the most frequent word, adding to the list \n",
    "                # and update the index of sumOfCol for not considering it again \n",
    "                if sumOfCol[i] == max_val:\n",
    "                    specific_word.append(wordList[i])\n",
    "                    sumOfCol[i] = 0\n",
    "        # when the length of the list reached to 10, break the the loop\n",
    "        if len(specific_word) == 10: \n",
    "            break\n",
    "    specific_word_for_each_category.append(specific_word)\n",
    "    print(categories[c], \": \", specific_word)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54dc530e",
   "metadata": {},
   "source": [
    "Output of First Perspective of Listing 10 words whose presence most strongly predicts:\n",
    "\n",
    "business :  ['in', 'and', 'said', 'that', 'is', 'it', 'for', 'on', 'has', 'its']\n",
    "    \n",
    "tech :  ['of', 'in', 'that', 'is', 'it', 'for', 'said', 'on', 'be', 'are']\n",
    "    \n",
    "politics :  ['of', 'and', 'he', 'said', 'mr', 'for', 'on', 'that', 'is', 'be']\n",
    "    \n",
    "sport :  ['to', 'in', 'and', 'of', 'he', 'is', 'we', 'for', 'it', 'on']\n",
    "    \n",
    "entertainment :  ['to', 'and', 'in', 'of', 'for', 'film', 'on', 'was', 'it', 'he']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805f99c0",
   "metadata": {},
   "source": [
    "# Second perspective of Listing 10 words whose presence most strongly predicts\n",
    "\n",
    "In this part we include words even if their idf is 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b9fec3c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "business :  ['the', 'to', 'in', 'of', 'and', 'said', 'that', 'is', 'it', 'for']\n",
      "tech :  ['the', 'to', 'of', 'and', 'in', 'that', 'is', 'it', 'for', 'said']\n",
      "politics :  ['the', 'to', 'of', 'and', 'in', 'he', 'said', 'mr', 'for', 'on']\n",
      "sport :  ['the', 'to', 'in', 'and', 'of', 'he', 'is', 'we', 'for', 'it']\n",
      "entertainment :  ['the', 'to', 'and', 'in', 'of', 'for', 'film', 'on', 'was', 'it']\n"
     ]
    }
   ],
   "source": [
    "vectorizer3 = TfidfVectorizer()\n",
    "specific_word_for_each_category = []\n",
    "for c in range(len(whole_category)):\n",
    "    vectorizer3 = TfidfVectorizer()\n",
    "    X = vectorizer3.fit_transform(whole_category[c])\n",
    "    idfs = vectorizer3.idf_  \n",
    "\n",
    "    wordList = vectorizer3.get_feature_names_out()\n",
    "\n",
    "    tfidf_matrix = X.toarray()\n",
    "\n",
    "    specific_word = []\n",
    "\n",
    "    sumOfCol = np.sum(X.toarray(), axis=0)\n",
    "\n",
    "    for k in range(15):  # this range should be at least the length of the list (10)\n",
    "        max_val = np.max(sumOfCol)\n",
    "\n",
    "        for i in range(len(wordList)):\n",
    "            \n",
    "            if max_val != 0:\n",
    "                if sumOfCol[i] == max_val:\n",
    "                    specific_word.append(wordList[i])\n",
    "                    sumOfCol[i] = 0\n",
    "\n",
    "        if len(specific_word) == 10:\n",
    "            break\n",
    "    specific_word_for_each_category.append(specific_word)\n",
    "    print(categories[c], \": \", specific_word)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54441406",
   "metadata": {},
   "source": [
    "Output Of Second perspective of Listing 10 words whose presence most strongly predicts:\n",
    "\n",
    "business :  ['the', 'to', 'in', 'of', 'and', 'said', 'that', 'is', 'it', 'for']\n",
    "    \n",
    "tech :  ['the', 'to', 'of', 'and', 'in', 'that', 'is', 'it', 'for', 'said']\n",
    "    \n",
    "politics :  ['the', 'to', 'of', 'and', 'in', 'he', 'said', 'mr', 'for', 'on']\n",
    "    \n",
    "sport :  ['the', 'to', 'in', 'and', 'of', 'he', 'is', 'we', 'for', 'it']\n",
    "    \n",
    "entertainment :  ['the', 'to', 'and', 'in', 'of', 'for', 'film', 'on', 'was', 'it']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78357b1c",
   "metadata": {},
   "source": [
    "# List the 10 words whose absence most strongly predicts \n",
    "\n",
    "Explaining logic of finding 10 words whose absence most strongly predicts is that:\n",
    "when idf of a word has max value, then this word appears rarely in the list \n",
    "and when sum of columns of tf-idf has min value, then it shows this word appearing rarely "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7c701965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "business :  ['accessing', 'adequate', 'boundary', 'bracing', 'brighten', 'calculate', 'carefully', 'chairs', 'checked', 'circuits']\n",
      "tech :  ['amazed', 'astro', 'brits', 'cant', 'coffee', 'compass', 'diamond', 'divides', 'fizzy', 'froth']\n",
      "politics :  ['20p', '3rds', '3x', '5000', '50pc', '75p', '80s', 'absorb', 'adair', 'afloat']\n",
      "sport :  ['aggression', 'applaud', 'avoids', 'bangs', 'boil', 'buoyed', 'buts', 'characteristic', 'chases', 'cheering']\n",
      "entertainment :  ['11m', '42m', '60s', 'abating', 'admission', 'advances', 'advert', 'affection', 'airbrushes', 'amateurish']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "absence_word_for_each_category = []\n",
    "for c in range(len(whole_category)):\n",
    "    vectorizer3 = TfidfVectorizer()\n",
    "    Y = vectorizer3.fit_transform(whole_category[c])\n",
    "    idfs = vectorizer3.idf_ - 1\n",
    "\n",
    "    wordList = vectorizer3.get_feature_names_out()\n",
    "\n",
    "    tfidf_mat = Y.toarray()\n",
    "\n",
    "    sumOfCol = np.sum(Y.toarray(), axis=0)\n",
    "\n",
    "    max_idf = np.max(idfs)\n",
    "\n",
    "    min_sum = np.min(sumOfCol)\n",
    "\n",
    "    absence_word = []\n",
    "\n",
    "    for i in range(len(wordList)):\n",
    "        if idfs[i] == max_idf:\n",
    "            if sumOfCol[i] == min_sum:\n",
    "                absence_word.append(wordList[i])\n",
    "\n",
    "            if len(absence_word) == 10:\n",
    "                break\n",
    "    absence_word_for_each_category.append(absence_word)\n",
    "    print(categories[c], \": \", absence_word)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5204b9",
   "metadata": {},
   "source": [
    "Output of absecence list:\n",
    "\n",
    "business :  ['accessing', 'adequate', 'boundary', 'bracing', 'brighten', 'calculate', 'carefully', 'chairs', 'checked', 'circuits']\n",
    "    \n",
    "tech :  ['amazed', 'astro', 'brits', 'cant', 'coffee', 'compass', 'diamond', 'divides', 'fizzy', 'froth']\n",
    "    \n",
    "politics :  ['20p', '3rds', '3x', '5000', '50pc', '75p', '80s', 'absorb', 'adair', 'afloat']\n",
    "    \n",
    "sport :  ['aggression', 'applaud', 'avoids', 'bangs', 'boil', 'buoyed', 'buts', 'characteristic', 'chases', 'cheering']\n",
    "    \n",
    "entertainment :  ['11m', '42m', '60s', 'abating', 'admission', 'advances', 'advert', 'affection', 'airbrushes', 'amateurish']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016246d3",
   "metadata": {},
   "source": [
    "# Removing Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "44b6cfe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "business :  ['year', 'mr', 'growth', 'sales', 'economy', 'market', 'bank', 'firm', 'oil', 'new']\n",
      "tech :  ['people', 'mobile', 'software', 'mr', 'phone', 'microsoft', 'music', 'games', 'net', 'users']\n",
      "politics :  ['mr', 'labour', 'election', 'blair', 'party', 'brown', 'government', 'people', 'tax', 'howard']\n",
      "sport :  ['england', 'game', 'year', 'world', 'win', 'time', 'wales', 'cup', 'ireland', 'chelsea']\n",
      "entertainment :  ['film', 'best', 'year', 'music', 'band', 'number', 'awards', 'new', 'award', 'won']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ENGLISH_STOP_WORDS = ENGLISH_STOP_WORDS.union(\n",
    "    [\"said\"])  # since every category has 'said', we carried 'said' to stop_word_list\n",
    "non_stop_specific_for_each_category = []\n",
    "for c in range(len(whole_category)):\n",
    "    vectorizer = TfidfVectorizer(stop_words=ENGLISH_STOP_WORDS)\n",
    "    X = vectorizer.fit_transform(whole_category[c])\n",
    "    idfs = vectorizer.idf_ - 1\n",
    "\n",
    "    wordList = vectorizer.get_feature_names_out()\n",
    "\n",
    "    tfidf_matrix = X.toarray()\n",
    "\n",
    "\n",
    "    specific_word = []\n",
    "\n",
    "    sumOfCol = np.sum(X.toarray(), axis=0)\n",
    "\n",
    "    for k in range(22):  # this range should be at least the length of the list (10)\n",
    "        max_val = np.max(sumOfCol)\n",
    "\n",
    "        for i in range(len(wordList)):\n",
    "            if max_val != 0:\n",
    "                if sumOfCol[i] == max_val:\n",
    "                    specific_word.append(wordList[i])\n",
    "                    sumOfCol[i] = 0\n",
    "\n",
    "        if len(specific_word) == 10:\n",
    "            break\n",
    "    non_stop_specific_for_each_category.append(specific_word)\n",
    "    print(categories[c], \": \", specific_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5615a4",
   "metadata": {},
   "source": [
    "Output of non-stop words list:\n",
    "    \n",
    "business :  ['year', 'mr', 'growth', 'sales', 'economy', 'market', 'bank', 'firm', 'oil', 'new']\n",
    "    \n",
    "tech :  ['people', 'mobile', 'software', 'mr', 'phone', 'microsoft', 'music', 'games', 'net', 'users']\n",
    "    \n",
    "politics :  ['mr', 'labour', 'election', 'blair', 'party', 'brown', 'government', 'people', 'tax', 'howard']\n",
    "    \n",
    "sport :  ['england', 'game', 'year', 'world', 'win', 'time', 'wales', 'cup', 'ireland', 'chelsea']\n",
    "    \n",
    "entertainment :  ['film', 'best', 'year', 'music', 'band', 'number', 'awards', 'new', 'award', 'won']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e666b13",
   "metadata": {},
   "source": [
    "# Reimplementing naive bayes Algorithm for part 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "45080b57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of part2 naive bayes unigram : 99.32885906040268\n",
      "Accuracy of part2 naive bayes bigram : 100.0\n",
      "Accuracy of part3 naive bayes unigram only using precences : 98.65771812080537\n",
      "Accuracy of part3 naive bayes bigram only using precences : 98.99328859060402\n",
      "Accuracy of part3 naive bayes unigram only using absences : 99.32885906040268\n",
      "Accuracy of part3 naive bayes bigram only using absences : 98.32214765100672\n",
      "Accuracy of part3 naive bayes unigram using both absences and presences : 98.65771812080537\n",
      "Accuracy of part3 naive bayes bigram using both absences and presences : 97.31543624161074\n",
      "Accuracy of part3 naive bayes unigram only using nonstop presences : 99.32885906040268\n",
      "Accuracy of part3 naive bayes bigram only using nonstop presences : 94.63087248322148\n"
     ]
    }
   ],
   "source": [
    "def improved_naive_bayes(unigrams,bigrams,test,specific_words,absence_words):\n",
    "    test = np.ndarray.tolist(test)\n",
    "    import re\n",
    "    global number_of_unique_words, length_of_words_categorically\n",
    "    for i in range(len(test)):\n",
    "        test[i] = test[i][0].lower()\n",
    "        test[i] = re.sub(r'[^a-zA-Z0-9]', ' ', test[i])\n",
    "        test[i] = test[i].split()\n",
    "    predictions_unigram = []\n",
    "    predictions_bigram = []\n",
    "    for i in range(len(test)):\n",
    "        scores_unigram = [0,0,0,0,0]\n",
    "        scores_bigram = [0,0,0,0,0]\n",
    "        for k in range(5):\n",
    "            for unique_word in set(test[i]):\n",
    "                if unique_word in specific_words[k]:\n",
    "                    # because when we look at the final scores of each text \n",
    "                    # for each category with bag of unigram\n",
    "                    # difference between min score and max score is in general \n",
    "                    # about half of the length of text\n",
    "                    # and in general every presence word is in text. So , this will work 10 times\n",
    "                    #  so if an absence word of a class is in a text\n",
    "                    # we are decreasing the probability of the class but we are not doing it impossible.\n",
    "                    scores_unigram[k] += len(test[i]) * 0.05\n",
    "                    # because when we look at the final scores of each text \n",
    "                    # for each category with bag of bigram\n",
    "                    # difference between min score and max score is in general \n",
    "                    # about 7 times of the length of text\n",
    "                    # and in general every presence word is in text. So , this will work 10 times\n",
    "                    # so if an absence word of a class is in a text\n",
    "                    # we are decreasing the probability of the class but we are not doing it impossible.\n",
    "                    scores_bigram[k] += len(test[i]) * 0.35\n",
    "            for j in range(len(test[i])):\n",
    "                if test[i][j] in absence_words[k]:\n",
    "                    # because when we look at the final scores of each text \n",
    "                    # for each category with bag of unigram\n",
    "                    # difference between min score and max score is in general about half of\n",
    "                    # the length of text so if an absence word of a class is in a text\n",
    "                    # we are decreasing the probability of the class but we are not doing it impossible.\n",
    "                    scores_unigram[k] -= len(test[i]) * 0.5\n",
    "                    # because when we look at the final scores of each text \n",
    "                    # for each category with bag of bigram\n",
    "                    # difference between min score and max score is in general \n",
    "                    # about 7 times of the length of text\n",
    "                    # so if an absence word of a class is in a text\n",
    "                    # we are decreasing the probability of the class but we are not doing it impossible.\n",
    "                    scores_bigram[k] -= len(test[i]) * 3.5\n",
    "                if test[i][j] in unigrams[categories[k]].keys():\n",
    "                    scores_unigram[k] += np.log((unigrams[categories[k]][test[i][j]] +1) / (length_of_words_categorically[k] + number_of_unique_words))\n",
    "                else:\n",
    "                    scores_unigram[k] += np.log(1 / (length_of_words_categorically[k] + number_of_unique_words))\n",
    "                if j+2 < len(test[i]):\n",
    "                    bigram = test[i][j] + ' ' + test[i][j+1]\n",
    "                    if bigram in bigrams[categories[k]].keys():\n",
    "                        scores_bigram[k] += np.log((bigrams[categories[k]][bigram] + 1) / (length_of_words_categorically[k] + number_of_unique_words))\n",
    "                    else:\n",
    "                        scores_bigram[k] += np.log(1/(length_of_words_categorically[k]+number_of_unique_words))\n",
    "        predictions_unigram.append(categories[scores_unigram.index(max(scores_unigram))])\n",
    "        predictions_bigram.append(categories[scores_bigram.index(max(scores_bigram))])\n",
    "    return predictions_unigram , predictions_bigram\n",
    "\n",
    "predictions = naive_bayes(bag_of_unigrams,bag_of_bigrams,test)\n",
    "print(\"Accuracy of part2 naive bayes unigram :\" ,accuracy(test[:,1],predictions[0]))\n",
    "print(\"Accuracy of part2 naive bayes bigram :\" ,accuracy(test[:,1],predictions[1]))\n",
    "predictions2 = improved_naive_bayes(bag_of_unigrams,bag_of_bigrams,test,[[],[],[],[],[]],absence_word_for_each_category)\n",
    "print(\"Accuracy of part3 naive bayes unigram only using precences :\" ,accuracy(test[:,1],predictions2[0]))\n",
    "print(\"Accuracy of part3 naive bayes bigram only using precences :\" ,accuracy(test[:,1],predictions2[1]))\n",
    "predictions3 = improved_naive_bayes(bag_of_unigrams,bag_of_bigrams,test,specific_word_for_each_category,[[],[],[],[],[]])\n",
    "print(\"Accuracy of part3 naive bayes unigram only using absences :\" ,accuracy(test[:,1],predictions3[0]))\n",
    "print(\"Accuracy of part3 naive bayes bigram only using absences :\" ,accuracy(test[:,1],predictions3[1]))\n",
    "predictions4 = improved_naive_bayes(bag_of_unigrams,bag_of_bigrams,test,specific_word_for_each_category,absence_word_for_each_category)\n",
    "print(\"Accuracy of part3 naive bayes unigram using both absences and presences :\" ,accuracy(test[:,1],predictions4[0]))\n",
    "print(\"Accuracy of part3 naive bayes bigram using both absences and presences :\" ,accuracy(test[:,1],predictions4[1]))\n",
    "predictions5 = improved_naive_bayes(bag_of_unigrams,bag_of_bigrams,test,non_stop_specific_for_each_category,[[],[],[],[],[]])\n",
    "print(\"Accuracy of part3 naive bayes unigram only using nonstop presences :\" ,accuracy(test[:,1],predictions5[0]))\n",
    "print(\"Accuracy of part3 naive bayes bigram only using nonstop presences :\" ,accuracy(test[:,1],predictions5[1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c51ee9",
   "metadata": {},
   "source": [
    "# Output 1:\n",
    "\n",
    "Accuracy of part2 naive bayes unigram : 98.99328859060402\n",
    "\n",
    "Accuracy of part2 naive bayes bigram : 99.66442953020135\n",
    "\n",
    "Accuracy of part3 naive bayes unigram only using precences : 98.99328859060402\n",
    "\n",
    "Accuracy of part3 naive bayes bigram only using precences : 99.32885906040268\n",
    "\n",
    "Accuracy of part3 naive bayes unigram only using absences : 98.99328859060402\n",
    "\n",
    "Accuracy of part3 naive bayes bigram only using absences : 96.97986577181209\n",
    "\n",
    "Accuracy of part3 naive bayes unigram using both absences and presences : 98.99328859060402\n",
    "\n",
    "Accuracy of part3 naive bayes bigram using both absences and presences : 96.64429530201342\n",
    "\n",
    "Accuracy of part3 naive bayes unigram only using nonstop presences : 98.65771812080537\n",
    "\n",
    "Accuracy of part3 naive bayes bigram only using nonstop presences : 95.63758389261746\n",
    "\n",
    "# Output 2:\n",
    "\n",
    "Accuracy of part2 naive bayes unigram : 100.0\n",
    "\n",
    "Accuracy of part2 naive bayes bigram : 100.0\n",
    "\n",
    "Accuracy of part3 naive bayes unigram only using precences : 100.0\n",
    "\n",
    "Accuracy of part3 naive bayes bigram only using precences : 100.0\n",
    "\n",
    "Accuracy of part3 naive bayes unigram only using absences : 100.0\n",
    "\n",
    "Accuracy of part3 naive bayes bigram only using absences : 97.31543624161074\n",
    "\n",
    "Accuracy of part3 naive bayes unigram using both absences and presences : 100.0\n",
    "\n",
    "Accuracy of part3 naive bayes bigram using both absences and presences : 97.31543624161074\n",
    "\n",
    "Accuracy of part3 naive bayes unigram only using nonstop presences : 99.66442953020135\n",
    "\n",
    "Accuracy of part3 naive bayes bigram only using nonstop presences : 95.30201342281879\n",
    "\n",
    "# Output 3\n",
    "\n",
    "Accuracy of part2 naive bayes unigram : 99.32885906040268\n",
    "\n",
    "Accuracy of part2 naive bayes bigram : 100.0\n",
    "\n",
    "Accuracy of part3 naive bayes unigram only using precences : 98.65771812080537\n",
    "\n",
    "Accuracy of part3 naive bayes bigram only using precences : 98.99328859060402\n",
    "\n",
    "Accuracy of part3 naive bayes unigram only using absences : 99.32885906040268\n",
    "\n",
    "Accuracy of part3 naive bayes bigram only using absences : 98.32214765100672\n",
    "\n",
    "Accuracy of part3 naive bayes unigram using both absences and presences : 98.65771812080537\n",
    "\n",
    "Accuracy of part3 naive bayes bigram using both absences and presences : 97.31543624161074\n",
    "\n",
    "Accuracy of part3 naive bayes unigram only using nonstop presences : 99.32885906040268\n",
    "\n",
    "Accuracy of part3 naive bayes bigram only using nonstop presences : 94.63087248322148"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733900da",
   "metadata": {},
   "source": [
    "# Effect of removing stop-words when improving model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d2d84bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram:  99.66442953020135\n",
      "Bigram:  65.43624161073825\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(stop_words=ENGLISH_STOP_WORDS) # CountVectorizer for unigram matrix.\n",
    "vectorizer2 = CountVectorizer(ngram_range=(2, 2),stop_words=ENGLISH_STOP_WORDS)# CountVectorizer for bigram matrix.\n",
    "whole_matrix = vectorizer.fit_transform(training[:, 0]).toarray()\n",
    "number_of_unique_words = whole_matrix.shape[1]\n",
    "whole_matrix_of_bigrams = vectorizer2.fit_transform(training[:, 0]).toarray()\n",
    "number_of_unique_bigrams = whole_matrix_of_bigrams.shape[1]\n",
    "bag_of_unigrams = defaultdict()\n",
    "bag_of_bigrams = defaultdict()\n",
    "# according to unigram and bigram matrices, creating dictionaries respectively\n",
    "for c in range(len(whole_category)):\n",
    "    cat_name = whole_category[c]\n",
    "    X = vectorizer.fit_transform(cat_name)\n",
    "    unigram_matrix = X.toarray()\n",
    "    words = (vectorizer.get_feature_names_out())\n",
    "    bigram_matrix = vectorizer2.fit_transform(cat_name).toarray()\n",
    "    bigrams = vectorizer2.get_feature_names_out()\n",
    "    vocabulary = defaultdict(int) # vocabulary of unigrams for each category\n",
    "    vocabulary2 = defaultdict(int) # vocabulary of bigrams for each category\n",
    "    sumOfColumns = np.sum(unigram_matrix, axis=0)\n",
    "    # adding existing words to unigram vocabulary\n",
    "    for i in range(unigram_matrix.shape[0]):\n",
    "        for j in range(unigram_matrix.shape[1]):\n",
    "            if unigram_matrix[i][j] == 0:\n",
    "                continue\n",
    "            else:\n",
    "                vocabulary[words[j]] += unigram_matrix[i][j]\n",
    "    # collecting unigram vocabularies for each category\n",
    "    bag_of_unigrams[categories[c]] = vocabulary\n",
    "    # adding existing words to bigram vocabulary\n",
    "    for i in range(bigram_matrix.shape[0]):\n",
    "        for j in range(bigram_matrix.shape[1]):\n",
    "            if bigram_matrix[i][j] == 0:\n",
    "                continue\n",
    "            else:\n",
    "                vocabulary2[bigrams[j]] += bigram_matrix[i][j]\n",
    "    # collecting bigram vocabularies for each category\n",
    "    bag_of_bigrams[categories[c]] = vocabulary2\n",
    "    \n",
    "def naive_bayes(unigrams,bigrams,test):\n",
    "    test = np.ndarray.tolist(test)\n",
    "    import re\n",
    "    global number_of_unique_words, length_of_words_categorically\n",
    "    for i in range(len(test)):\n",
    "        test[i] = test[i][0].lower() # converting every letter to lowercase \n",
    "        test[i] = re.sub(r'[^a-zA-Z0-9]', ' ', test[i]) # we are removing punctuations for not considering them as a word\n",
    "        test[i] = test[i].split() # converting text string to string list of every word.\n",
    "    predictions_unigram = []\n",
    "    predictions_bigram = []\n",
    "    for i in range(len(test)):\n",
    "        scores_unigram = [0,0,0,0,0] # keeping scores of each category for unigram\n",
    "        scores_bigram = [0,0,0,0,0] # keeping scores of each category for bigram\n",
    "        for j in range(len(test[i])):\n",
    "            for k in range(5):\n",
    "                if test[i][j] in unigrams[categories[k]].keys():\n",
    "                    scores_unigram[k] += np.log((unigrams[categories[k]][test[i][j]] +1) / (length_of_words_categorically[k] + number_of_unique_words))\n",
    "                else:\n",
    "                    scores_unigram[k] += np.log(1 / (length_of_words_categorically[k] + number_of_unique_words))\n",
    "                if j+2 < len(test[i]):\n",
    "                    bigram = test[i][j] + ' ' + test[i][j+1]\n",
    "                    if bigram in bigrams[categories[k]].keys():\n",
    "                        scores_bigram[k] += np.log((bigrams[categories[k]][bigram] + 1) / (length_of_words_categorically[k] + number_of_unique_words))\n",
    "                    else:\n",
    "                        scores_bigram[k] += np.log(1/(length_of_words_categorically[k]+number_of_unique_words))\n",
    "\n",
    "        predictions_unigram.append(categories[scores_unigram.index(max(scores_unigram))])\n",
    "        predictions_bigram.append(categories[scores_bigram.index(max(scores_bigram))])\n",
    "    return predictions_unigram , predictions_bigram\n",
    "\n",
    "def accuracy(outputs,predictions):\n",
    "    correct_predictions = 0\n",
    "    for i in range(len(outputs)):\n",
    "        if outputs[i] == predictions[i]:\n",
    "            correct_predictions +=1\n",
    "    return 100 * correct_predictions / len(outputs)\n",
    "\n",
    "predictions = naive_bayes(bag_of_unigrams,bag_of_bigrams,test)\n",
    "print(\"Unigram: \",accuracy(test[:,1],predictions[0]))\n",
    "print(\"Bigram: \",accuracy(test[:,1],predictions[1]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3603cf65",
   "metadata": {},
   "source": [
    "Output 1:\n",
    "\n",
    "Unigram:  99.66442953020135\n",
    "    \n",
    "Bigram:  65.43624161073825\n",
    "    \n",
    "Output 2:\n",
    "\n",
    "Unigram:  99.66442953020135\n",
    "    \n",
    "Bigram:  69.79865771812081\n",
    "    \n",
    "Output 3:\n",
    "\n",
    "Unigram:  99.32885906040268\n",
    "    \n",
    "Bigram:  71.81208053691275"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a756e6",
   "metadata": {},
   "source": [
    "The reason behind decreasing accuracy rates of bigram is that when we remove stopwords \n",
    "from the entire dataset, unrelated word pairs will increase."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265aae08",
   "metadata": {},
   "source": [
    "# Part 4 accuracy algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33425c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(outputs,predictions):\n",
    "    correct_predictions = 0\n",
    "    for i in range(len(outputs)):\n",
    "        if outputs[i] == predictions[i]:\n",
    "            correct_predictions +=1\n",
    "    return 100 * correct_predictions / len(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfeaaae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
